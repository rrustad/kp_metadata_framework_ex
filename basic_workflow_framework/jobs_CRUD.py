# Databricks notebook source
import json
import requests

# COMMAND ----------

dbutils.widgets.dropdown('init', 'False', ['True', 'False'])
init = dbutils.widgets.get('init') == 'True'
dbutils.widgets.dropdown('test', 'False', ['True', 'False'])
test = dbutils.widgets.get('test') == 'True'

dbutils.widgets.text('source_catalog', 'kp_catalog')
source_catalog = dbutils.widgets.get('source_catalog')
dbutils.widgets.text('checkpoints_path', 'abfss://kp-external-location@oneenvadls.dfs.core.windows.net/clarity_ingest/checkpoints/')
checkpoints_path = dbutils.widgets.get('checkpoints_path')
dbutils.widgets.text('incremental_notebook_path', '/Repos/riley.rustad@databricks.com/kp_metadata_framework_ex/basic_workflow_framework/get_type1_data')
incremental_notebook_path = dbutils.widgets.get('incremental_notebook_path')
dbutils.widgets.text('status_notebook_path', '/Repos/riley.rustad@databricks.com/kp_metadata_framework_ex/basic_workflow_framework/check_status')
status_notebook_path = dbutils.widgets.get('status_notebook_path')

# COMMAND ----------

if init:
  spark.sql(f"""create or replace table {source_catalog}.clarity_stg.table_meta (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY, 
    app_id STRING, 
    primary_keys STRING, 
    source_schema STRING, 
    source_table STRING, 
    delete_table STRING, 
    target_schema STRING, 
    target_table STRING,
    timeout_seconds INTEGER)""")
  df = spark.createDataFrame([
    ["1","tb1pk1","clarity_stg","table1","table1_del","clarity_enr","table1",3600],
    ["1","tb2pk1,tb2pk2","clarity_stg","table2","table1_del","clarity_enr","table2",3600],
    ["2","tb3pk1,tb3pk2","clarity_stg","table3","table1_del","clarity_enr","table3",3600],
    ["2","tb4pk1","clarity_stg","table4","table1_del","clarity_enr","table4",3600],
    ["2","tb5pk1,tb5pk2,tb5pk3","clarity_stg","table5","table1_del","clarity_enr","table5",3600],
  ], schema="app_id string, primary_keys string, source_schema string, source_table string, delete_table string, target_schema string, target_table string, timeout_seconds integer")
  df.write.mode('append').saveAsTable(f'{source_catalog}.clarity_stg.table_meta')

# COMMAND ----------

with open('./clusters.json','r') as f:
  clusters = json.load(f)
clusters

# COMMAND ----------

tables = spark.table(f'{source_catalog}.clarity_stg.table_meta')
tables.display()

# COMMAND ----------

tables_pd = tables.toPandas()
tables_pd

# COMMAND ----------

if test:
  assert set(tables_pd['app_id'].unique()) == set(clusters.keys())

# COMMAND ----------

clusters[app]

# COMMAND ----------


url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None) 
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)


_token = {
            'Authorization': 'Bearer {0}'.format(token),
        }

# COMMAND ----------

for app in tables_pd['app_id'].unique():
  app_tables = tables_pd[tables_pd['app_id']==app]
  tasks = []
  cluster_config = clusters[app]
  for row in app_tables.iterrows():
    id_ = row[1]['id']
    app_id = row[1]['app_id']
    primary_keys = row[1]['primary_keys']
    source_schema = row[1]['source_schema']
    source_table = row[1]['source_table']
    delete_table = row[1]['delete_table']
    target_schema = row[1]['target_schema']
    target_table = row[1]['target_table']
    timeout_seconds = row[1]['timeout_seconds']
    # Create Tasks
    tasks.append({
      "task_key": f"{source_table}_status_check",
      "notebook_task": {
        "notebook_path": status_notebook_path,
        "base_parameters": {
          "source_catalog": source_catalog,
          "source_table": source_table
        },
        "source": "WORKSPACE"
      },
      "job_cluster_key": f"{app_id}_status_cluster",
      "timeout_seconds": 14400,
      "health": {
        "rules": [
          {
            "metric": "RUN_DURATION_SECONDS",
            "op": "GREATER_THAN",
            "value": 3600
          }
        ]
      },
    })

    tasks.append({
      "task_key": f"{source_table}_update",
      "depends_on": [
        {
          "task_key": f"{source_table}_status_check"
        }
        ],
      "run_if": "ALL_SUCCESS",
      "notebook_task": {
        "notebook_path": incremental_notebook_path,
        "base_parameters": {
          "source_catalog":source_catalog,
          "source_table_path":f"{source_catalog}.{source_schema}.{source_table}",
          "target_table_path":f"{source_catalog}.{target_schema}.{target_table}",
          "primary_keys":primary_keys,
          "audit_table_path":f"{source_catalog}.{source_schema}.table_status",
          "delete_table_path":f"{source_catalog}.{source_schema}.{source_table}_del",
          "table_name":source_table,
          "checkpoints_path":checkpoints_path
        },
        "source": "WORKSPACE"
      },
      "job_cluster_key": f"{app_id}_ingest_cluster",
      "timeout_seconds": timeout_seconds,
      "health": {
        "rules": [
          {
            "metric": "RUN_DURATION_SECONDS",
            "op": "GREATER_THAN",
            "value": timeout_seconds/2
          }
        ]
      },
    })

  # Create Job Def

    job_def = {
    "name": f"clarity_ingest_{app_id}",
    "tasks":tasks,
    "job_clusters": [
    {
      "job_cluster_key": f"{app_id}_ingest_cluster",
      "new_cluster": {
        "spark_version": cluster_config['spark_version'],
        "instance_pool_id": cluster_config['worker_pool_id'],
        "policy_id": cluster_config['policy_id'],
        "driver_instance_pool_id": cluster_config['driver_pool_id'],
        "data_security_mode": "USER_ISOLATION",
        "runtime_engine": "PHOTON",
        "num_workers": cluster_config['num_workers']
      }
    },
    {
      "job_cluster_key": f"{app_id}_status_cluster",
      "new_cluster": {
        "spark_version": cluster_config['spark_version'],
        "instance_pool_id": cluster_config['worker_pool_id'],
        "policy_id": cluster_config['policy_id'],
        "driver_instance_pool_id": cluster_config['driver_pool_id'],
        "data_security_mode": "USER_ISOLATION",
        "runtime_engine": "PHOTON",
        "num_workers": 1
      }
    }
  ]
  }

  r = requests.post(url + '/api/2.1/jobs/create', headers=_token, data = json.dumps(job_def))
  with open(f"./workflows/{app_id}.json","w") as f:
    json.dump(job_def, f, indent=2)
  


# COMMAND ----------


# r = requests.post(url + '/api/2.1/jobs/create', headers=_token, data = json.dumps(job_def))
# r.json()

# COMMAND ----------

# TODO: Implement job update

# COMMAND ----------

# TODO: Implement job delete
